<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>GAN_代码生成 | have a good time!</title><meta name="author" content="mervin"><meta name="copyright" content="mervin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="基于tensorflow依赖 3个外部库：tensorflow、numpy、matplotlib 用Sequential搭网络，用Model做GAN的链式模型？？？  超参不会在反向传播里更新，但直接决定模型能否收敛、生成质量、训练速度 这些参数是要可重复，为了调参方便，可自动化搜索  [!IMPORTANT] 这三个参数是生成对抗网络（GAN）或深度学习模型训练中最核心的超参数，常用于控制模型的">
<meta property="og:type" content="article">
<meta property="og:title" content="GAN_代码生成">
<meta property="og:url" content="http://example.com/2025/10/17/GAN-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/index.html">
<meta property="og:site_name" content="have a good time!">
<meta property="og:description" content="基于tensorflow依赖 3个外部库：tensorflow、numpy、matplotlib 用Sequential搭网络，用Model做GAN的链式模型？？？  超参不会在反向传播里更新，但直接决定模型能否收敛、生成质量、训练速度 这些参数是要可重复，为了调参方便，可自动化搜索  [!IMPORTANT] 这三个参数是生成对抗网络（GAN）或深度学习模型训练中最核心的超参数，常用于控制模型的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://p111223.github.io/imgs/2025-05-01%20005434.png">
<meta property="article:published_time" content="2025-10-17T07:05:24.000Z">
<meta property="article:modified_time" content="2025-10-21T11:05:45.735Z">
<meta property="article:author" content="mervin">
<meta property="article:tag" content="深度伪造">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://p111223.github.io/imgs/2025-05-01%20005434.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "GAN_代码生成",
  "url": "http://example.com/2025/10/17/GAN-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/",
  "image": "https://p111223.github.io/imgs/2025-05-01 005434.png",
  "datePublished": "2025-10-17T07:05:24.000Z",
  "dateModified": "2025-10-21T11:05:45.735Z",
  "author": [
    {
      "@type": "Person",
      "name": "mervin",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://example.com/2025/10/17/GAN-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'GAN_代码生成',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(https://p111223.github.io/imgs/avatar-icon.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://p111223.github.io/imgs/2025-05-01 005434.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="https://p111223.github.io/imgs/2025-05-19 230655.png" alt="Logo"><span class="site-name">have a good time!</span></a><a class="nav-page-title" href="/"><span class="site-name">GAN_代码生成</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">GAN_代码生成</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-10-17T07:05:24.000Z" title="发表于 2025-10-17 15:05:24">2025-10-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-21T11:05:45.735Z" title="更新于 2025-10-21 19:05:45">2025-10-21</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="基于tensorflow"><a href="#基于tensorflow" class="headerlink" title="基于tensorflow"></a>基于tensorflow</h1><h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><ul>
<li>3个外部库：<code>tensorflow</code>、<code>numpy</code>、<code>matplotlib</code></li>
<li>用<code>Sequential</code>搭网络，用<code>Model</code>做GAN的链式模型？？？</li>
</ul>
<h2 id="超参"><a href="#超参" class="headerlink" title="超参"></a>超参</h2><p>不会在反向传播里更新，<strong>但直接决定模型能否收敛、生成质量、训练速度</strong></p>
<p>这些参数是要<strong>可重复</strong>，为了<strong>调参方便</strong>，可<strong>自动化搜索</strong></p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>这三个参数是<strong>生成对抗网络（GAN）或深度学习模型训练中最核心的超参数</strong>，常用于控制模型的训练过程和输入数据形态，简要解析如下：</p>
<ol>
<li><p><strong>EPOCHS &#x3D; 50</strong><br>“EPOCHS”（迭代次数）指<strong>整个训练数据集被模型完整训练一遍的次数</strong>。  </p>
<ul>
<li>50表示训练时，所有训练样本会被模型“过”50次；  </li>
<li>作用：太少会导致模型“欠拟合”（没学会数据规律），太多可能导致“过拟合”（只记住训练数据、泛化能力差），50是常见的中等迭代次数，适用于数据量适中、模型复杂度一般的场景。</li>
</ul>
</li>
<li><p><strong>BATCH_SIZE &#x3D; 128</strong><br>“BATCH_SIZE”（批次大小）指<strong>每次模型参数更新时，输入的样本数量</strong>。  </p>
<ul>
<li>128表示每次训练，模型会同时处理128个样本，计算这128个样本的平均误差后，再调整一次参数；  </li>
<li>作用：平衡训练效率和稳定性——批次太小会导致参数更新震荡（不稳定）、训练慢；批次太大可能占用过多显存，且单次更新对整体数据的代表性下降，128是深度学习中兼顾效率与稳定性的常用值。</li>
</ul>
</li>
<li><p><strong>NOISE_DIM &#x3D; 100</strong><br>“NOISE_DIM”（噪声维度）是<strong>生成模型（如GAN的生成器）的核心输入参数</strong>，仅用于需要“从随机噪声生成数据”的场景（如生成图片、文本等）。  </p>
<ul>
<li>100表示生成器的输入是一个“100维的随机向量”（如符合正态分布的随机数）；  </li>
<li>作用：这个高维噪声向量是生成器的“创意源头”，模型通过学习将100维的无序噪声，映射为有序的目标数据（如一张256x256的图片），100是生成类模型中常见的噪声维度（维度太低会限制生成多样性，太高会增加模型学习难度）。</li>
</ul>
</li>
</ol>
</blockquote>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>设置函数<code>load_data()</code></p>
<ul>
<li>相当于无监督学习，不用<code>y_train</code>（标签）</li>
<li>归一化到[-1,1]，因为生成器最后一层是<code>tanh</code>，输出范围也是[-1,1]</li>
</ul>
<blockquote>
<p>[!IMPORTANT]</p>
<p>这个函数<code>load_data()</code>的作用是加载并预处理MNIST手写数字数据集，主要步骤解析如下：</p>
<ol>
<li><p>通过<code>mnist.load_data()</code>加载数据集，该函数返回格式为<code>(训练集, 测试集)</code>，其中每个集合又包含<code>(图像数据, 标签)</code>。这里用<code>(x_train, _), (_, _)</code>只保留训练集的图像数据<code>x_train</code>，忽略训练集标签和测试集的所有数据（<strong>用<code>_</code>表示不需要的变量</strong>）。</p>
</li>
<li><p>对<code>x_train</code>进行归一化处理：<code>(x_train.astype(&quot;float32&quot;) - 127.5) / 127.5</code>。MNIST图像像素值范围是0-255，减去127.5再除以127.5后，像素值会被映射到[-1, 1]区间，这是很多生成模型（如GAN）常用的预处理方式。</p>
</li>
<li><p>重塑数据形状：<code>x_train.reshape(-1, 784)</code>。MNIST图像原始形状是28×28的二维数组，这里将其展平为784个元素的一维数组（<code>-1</code>表示自动计算该维度的大小，保持样本数量不变），便于后续输入模型进行处理。</p>
</li>
</ol>
<p>最终函数返回预处理后的训练集图像数据，可直接用于模型训练。</p>
</blockquote>
<h2 id="模型构件"><a href="#模型构件" class="headerlink" title="模型构件"></a>模型构件</h2><h3 id="build-generator"><a href="#build-generator" class="headerlink" title="build_generator()"></a>build_generator()</h3><p>用<code>Sequential()</code>创建模型，再通过<code>.compile(loss, optimizor)</code>（参数了解一下），做预准备</p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>这段代码定义了一个生成对抗网络（GAN）中的生成器模型，主要功能是将随机噪声转换为类似目标数据（结合输出维度784推测可能是28x28像素的灰度图像，如MNIST数据集）的伪造数据。以下是简要解析：</p>
<ol>
<li><p><strong>模型结构</strong>：</p>
<ul>
<li>采用<code>Sequential</code>顺序模型，由<strong>4个全连接层</strong>（<code>Dense</code>）组成</li>
<li>输入为维度<code>NOISE_DIM</code>的随机噪声（噪声向量）</li>
<li>前3层使用<code>LeakyReLU</code>激活函数（alpha&#x3D;0.2，控制负斜率），逐步提升维度（256→512→1024），增强特征表达能力</li>
<li><strong>输出层为784维</strong>（对应28x28图像展平后的维度），使用<code>tanh</code>激活函数，将输出值映射到[-1, 1]范围（符合图像数据归一化常见处理）</li>
</ul>
</li>
<li><p><strong>编译配置</strong>：<code>.compile()</code></p>
<ul>
<li>损失函数采用<code>binary_crossentropy</code>（二元交叉熵），这是GAN中生成器与判别器对抗训练的常用损失</li>
<li>优化器为<code>Adam</code>，学习率0.0002，beta_1&#x3D;0.5，是GAN训练中经过实践验证的较优参数组合（较小的beta_1使动量估计更关注近期梯度）</li>
</ul>
</li>
</ol>
<p>整体来看，该生成器通过多层全连接网络对噪声进行非线性变换，最终生成与目标数据分布相似的输出，是典型的GAN生成器架构。</p>
</blockquote>
<p>100维噪音 &#x3D;&#x3D;&gt; 256 &#x3D;&#x3D;&gt; 512 &#x3D;&#x3D;&gt; 1024 &#x3D;&#x3D;&gt; 784（假图）</p>
<h3 id="build-discriminator"><a href="#build-discriminator" class="headerlink" title="build_discriminator()"></a>build_discriminator()</h3><p>同样的方式，参数内容不同</p>
<p>多了个Dropout(0.3)来防止过拟合，</p>
<blockquote>
<p>[!IMPORTANT]</p>
<p>这段代码定义了一个用于生成对抗网络（GAN）中的判别器模型，功能是判断输入数据是真实样本还是生成器生成的假样本，具体解析如下：</p>
<ol>
<li><p><strong>模型结构</strong>：</p>
<ul>
<li>采用<code>Sequential</code>顺序模型，由多个全连接层（<code>Dense</code>）堆叠而成</li>
<li>输入层为<code>Dense(1024, input_dim=784)</code>，说明输入数据是784维（对应MNIST数据集的28×28像素图像展平后的数据），输出1024维特征</li>
<li>中间层依次为512维、256维的全连接层，每层后均使用<code>LeakyReLU(alpha=0.2)</code>激活函数（解决ReLU的死亡神经元问题，小斜率0.2允许少量负输入通过）</li>
<li><strong>各层间加入<code>Dropout(0.3)</code>，随机丢弃30%的神经元，防止过拟合</strong></li>
<li>**输出层为<code>Dense(1, activation=&quot;sigmoid&quot;)</code>**，输出单个0-1之间的值（1表示判断为真实样本，0表示判断为假样本）</li>
</ul>
</li>
<li><p><strong>模型配置</strong>：</p>
<ul>
<li>使用<code>binary_crossentropy</code>（二元交叉熵）损失函数，适合二分类任务（真实&#x2F;假样本判断）</li>
<li>优化器为<code>Adam</code>，学习率0.0002，<code>beta_1=0.5</code>（GAN中常用的参数设置，有助于稳定训练）</li>
</ul>
</li>
</ol>
<p>整体来看，这是一个典型的GAN判别器架构，通过多层全连接网络提取特征，最终输出对输入数据真实性的判断概率。</p>
</blockquote>
<h3 id="build-gan"><a href="#build-gan" class="headerlink" title="build_gan"></a>build_gan</h3><p>作为“生成器部分”的训练通路</p>
<ul>
<li><p>噪音 &#x3D;&#x3D;&gt; 生成器 &#x3D;&#x3D;&gt; 假图 &#x3D;&#x3D;&gt; 判别器 &#x3D;&#x3D;&gt; 分数，最后<code>Model</code>生成gan，并<code>.compile</code></p>
</li>
<li><p>设置判别器<code>trainable=False</code>保证<strong>反向传播只更新生成器权重</strong></p>
</li>
</ul>
<blockquote>
<p>[!IMPORTANT]</p>
<p>这段代码定义了一个构建生成对抗网络（GAN）的函数<code>build_gan</code>，主要作用是将生成器（<code>gen</code>）和判别器（<code>disc</code>）组合成一个完整的GAN模型，用于训练生成器。以下是简要解析：</p>
<ol>
<li><p><strong>冻结判别器参数</strong>：<code>disc.trainable = False</code><br>这一步是关键，在训练GAN（即训练生成器时），需要固定判别器的参数，避免其在生成器的训练过程中被更新，确保生成器的优化目标是“欺骗”当前状态的判别器。</p>
</li>
<li><p><strong>定义GAN的输入和输出</strong>：  </p>
<ul>
<li><code>gan_input = Input(shape=(NOISE_DIM,))</code>：定义GAN的输入为噪声向量（维度为<code>NOISE_DIM</code>），这也是生成器的输入。  </li>
<li><code>gan_output = disc(gen(gan_input))</code>：将噪声输入生成器得到伪造样本，再将伪造样本输入判别器，得到判别器的输出（即对伪造样本的“真假判断”），<strong>这是GAN的最终输出。</strong></li>
</ul>
</li>
<li><p><strong>构建并编译GAN模型</strong>：  </p>
<ul>
<li><code>gan = Model(gan_input, gan_output)</code>：以噪声为输入、判别器对伪造样本的判断为输出，构建GAN模型。  </li>
<li>编译时使用二元交叉熵损失（<code>binary_crossentropy</code>）和Adam优化器（学习率0.0002，动量参数<code>beta_1=0.5</code>，这是GAN训练中常用的参数设置）。</li>
</ul>
</li>
</ol>
<p>整体来看，这个函数的核心是将生成器和判别器串联，通过冻结判别器，让生成器在训练时专注于学习如何生成能“骗过”判别器的样本。</p>
</blockquote>
<p>？？？？？？这里有个疑惑，为什么要冻结disc，——这里不包含参数的更新；在train()函数中，有对应的模块进行训练disc、gen，这一步的操作意义好像没有，所以，对这一步不是很理解</p>
<p>看下面训练的操作可知了</p>
<h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><p><code>save_images()</code>函数</p>
<ul>
<li>10个epoch存一张图</li>
<li><code>matplotlib.use(&quot;Agg&quot;)</code>保证在纯命令行服务器也能跑</li>
<li>同时也存图</li>
</ul>
<blockquote>
<p>[!IMPORTANT]</p>
<p>这段代码定义了一个用于保存生成模型（如GAN中的生成器）生成图像的函数<code>save_images</code>，主要功能解析如下：</p>
<ol>
<li><p><strong>参数说明</strong>：</p>
<ul>
<li><code>gen</code>：生成模型（生成器），用于根据输入噪声生成图像</li>
<li><code>epoch</code>：当前训练轮次，用于命名保存的图片</li>
<li><code>examples</code>：生成图像的数量（默认25张）</li>
<li><code>dim</code>：图像排列的行列数（默认5x5网格，对应25张图）</li>
<li><code>figsize</code>：画布大小（默认6x6）</li>
</ul>
</li>
<li><p><strong>核心逻辑</strong>：</p>
<ul>
<li>生成符合正态分布的随机噪声（均值0，标准差1），形状为<code>(examples, NOISE_DIM)</code>（<code>NOISE_DIM</code>是噪声向量维度）</li>
<li>用生成器<code>gen</code>对噪声进行预测，得到生成图像，并将其reshape为<code>(examples, 28, 28)</code>（符合MNIST等手写数字数据集的图像尺寸）</li>
<li>使用matplotlib创建画布，按<code>dim</code>指定的网格排列图像</li>
<li>每张图像以灰度图（<code>cmap=&quot;gray&quot;</code>）显示，关闭坐标轴</li>
<li>调整布局后，将图像保存到<code>RESULT_DIR</code>目录，文件名格式为<code>generated_epoch_xxx.png</code>（xxx为3位数字的轮次）</li>
<li>关闭画布，避免内存占用</li>
</ul>
</li>
</ol>
<p>整体作用是在模型训练过程中，定期保存生成器的输出结果，方便观察模型生成效果随训练的变化。</p>
</blockquote>
<h2 id="train"><a href="#train" class="headerlink" title="train()"></a>train()</h2><p>整合、循环训练</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for epoch in 1~50:</span><br><span class="line">    for step in 0~468:        # 60000//128 ≈ 468 步</span><br><span class="line">        1. 采样真图 batch</span><br><span class="line">        2. 生成假图 batch</span><br><span class="line">        3. 拼在一起训练判别器（真标签=1，假标签=0）</span><br><span class="line">        4. 冻结判别器，训练组合模型 GAN：</span><br><span class="line">           输入噪声，标签却写“1”——</span><br><span class="line">           目的是让生成器学会“骗过”判别器</span><br><span class="line">    每 10 epoch 保存一次图片</span><br></pre></td></tr></table></figure>

<ul>
<li><p>one判别器、one生成器，交替进行</p>
</li>
<li><p>train过程中训练方式：</p>
<ul>
<li>gen模型<strong>仅是用来生成fake_imgs</strong></li>
<li>disc模型用来判断，会进行train_on_batch，两次，对应于real和fake</li>
<li>gan模型虽然是包含了上面两个模型，但在其开始会disc.trainable&#x3D;False，，所以<strong>链式模型 <code>gan</code> 的 <code>train_on_batch</code> 只更新生成器 G 的权重</strong>（链式模型 gan 天然只反向到 G，因为在build_gan时，通过<code>disc.trainable = False</code>实现不把D的权重放进gan的**<code>trainable_weights</code>**）——呼应build_gan的False操作</li>
</ul>
<p><strong>G 永远开放，D 动态上锁；</strong><br><strong>gan 只反到 G，权重列表作证。</strong></p>
</li>
</ul>
<blockquote>
<p>[!IMPORTANT]</p>
<p>这段代码是一个典型的生成对抗网络（GAN）训练流程，主要逻辑解析如下：</p>
<ol>
<li><p><strong>数据准备</strong>：</p>
<ul>
<li>加载训练数据<code>x_train</code>，计算数据量和每个epoch的训练步数（总数据量&#x2F;&#x2F;批次大小）</li>
</ul>
</li>
<li><p><strong>模型构建</strong>：</p>
<ul>
<li>初始化生成器（<code>gen</code>）、判别器（<code>disc</code>）</li>
<li>构建GAN整体模型（<code>gan</code>），将生成器和判别器结合</li>
</ul>
</li>
<li><p><strong>标签设置</strong>：</p>
<ul>
<li>真实样本标签设为1（<code>real_labels</code>）</li>
<li>生成样本标签设为0（<code>fake_labels</code>）</li>
</ul>
</li>
<li><p><strong>训练循环</strong>：</p>
<ul>
<li>外层循环按epoch迭代（从1到EPOCHS）</li>
<li>内层循环按步数迭代（每个epoch包含steps_per_epoch步）</li>
</ul>
</li>
<li><p><strong>判别器训练</strong>：</p>
<ul>
<li>生成随机噪声，通过生成器得到假样本（<code>fake_imgs</code>）</li>
<li>从训练集中随机抽取真实样本（<code>real_imgs</code>）</li>
<li>开启判别器训练模式，分别用真实样本和假样本训练，得到两个损失（<code>d_loss_real</code>和<code>d_loss_fake</code>）</li>
</ul>
</li>
<li><p><strong>生成器训练</strong>：</p>
<ul>
<li>冻结判别器参数（<code>disc.trainable = False</code>）</li>
<li>用噪声作为输入，以真实标签（1）为目标训练生成器，让判别器无法区分生成样本</li>
</ul>
</li>
<li><p><strong>训练监控与保存</strong>：</p>
<ul>
<li>每个epoch结束打印损失值</li>
<li>第1个epoch和每10个epoch保存生成的图片</li>
<li>训练结束后保存生成器和判别器的权重</li>
</ul>
</li>
</ol>
<p>核心思想是通过生成器和判别器的对抗训练（生成器试图生成逼真样本欺骗判别器，判别器试图区分真假样本），最终使生成器能够生成高质量的逼真样本。</p>
</blockquote>
<table>
<thead>
<tr>
<th>层级</th>
<th>决定的动作</th>
<th>直接影响的量（已有参数）</th>
<th>典型代码位置</th>
</tr>
</thead>
<tbody><tr>
<td><strong>step</strong>（微观）</td>
<td>1. 计算梯度<br>2. 执行权重更新</td>
<td><strong>全部模型权重</strong> <code>W, b</code></td>
<td><code>optimizer.apply_gradients()</code><br><code>model.train_on_batch()</code></td>
</tr>
<tr>
<td><strong>epoch</strong>（宏观）</td>
<td>1. 学习率调度<br>2. 早停<br>3. 保存权重<br>4. 打印&#x2F;可视化</td>
<td>- <strong>学习率</strong> <code>lr</code>（调度器）<br>- <strong>最优权重副本</strong> <code>best_model.*</code><br>- <strong>日志、图像、检查点</strong></td>
<td><code>ReduceLROnPlateau</code><br><code>ModelCheckpoint</code><br><code>save_images()</code><br><code>print(f&quot;Epoch &#123;e&#125; loss=&#123;loss:.4f&#125;&quot;)</code></td>
</tr>
</tbody></table>
<p><strong>step 决定“权重什么时候改”；epoch 决定“学习率什么时候降、权重什么时候存、日志什么时候打”</strong></p>
<h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><p><code>train()</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">真图 ──┐</span><br><span class="line">       ├→ 判别器 → 真/假分数 → 损失 → 更新判别器权重</span><br><span class="line">假图 ──┘  ↑</span><br><span class="line">         │ 由生成器产生</span><br><span class="line">         │        │</span><br><span class="line">噪声 → 生成器 → 假图 → 判别器 → 分数 → 损失 → 只更新生成器权重</span><br><span class="line">                                      (冻结判别器)</span><br></pre></td></tr></table></figure>

<p>生成两个<code>.h5</code>权重文件</p>
<p>和一些过程数字图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">基于MNIST图像集的GAN，基于tensorflow的代码</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.use(<span class="string">&quot;Agg&quot;</span>)          <span class="comment"># 无显示器也能画图，让matplotlib只在内存生成png，不写屏幕</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&quot;TF_CPP_MIN_LOG_LEVEL&quot;</span>] = <span class="string">&quot;2&quot;</span>   <span class="comment"># 屏蔽 TF INFO/WARN</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense, Dropout, LeakyReLU, Input</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Model, Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 1. 超参</span></span><br><span class="line">EPOCHS      = <span class="number">50</span> <span class="comment">#迭代次数——指整个训练数据集被模型完整训练一遍的次数</span></span><br><span class="line">BATCH_SIZE  = <span class="number">128</span> <span class="comment">#每次模型参数更新时，输入的样本数量</span></span><br><span class="line">NOISE_DIM   = <span class="number">100</span> <span class="comment">#噪声维度</span></span><br><span class="line">RESULT_DIR  = <span class="string">&quot;result&quot;</span></span><br><span class="line">os.makedirs(RESULT_DIR, exist_ok=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 2. 数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>():</span><br><span class="line">    (x_train, _), (_, _) = mnist.load_data()</span><br><span class="line">    x_train = (x_train.astype(<span class="string">&quot;float32&quot;</span>) - <span class="number">127.5</span>) / <span class="number">127.5</span> <span class="comment">#归一化处理</span></span><br><span class="line">    x_train = x_train.reshape(-<span class="number">1</span>, <span class="number">784</span>) <span class="comment">#28*28的二维数组，展平为784个元素的一位数组</span></span><br><span class="line">    <span class="keyword">return</span> x_train <span class="comment">#返回数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 3. 模型构件</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_generator</span>():</span><br><span class="line">    model = Sequential([</span><br><span class="line">        Dense(<span class="number">256</span>, input_dim=NOISE_DIM),</span><br><span class="line">        LeakyReLU(alpha=<span class="number">0.2</span>),</span><br><span class="line">        Dense(<span class="number">512</span>),</span><br><span class="line">        LeakyReLU(alpha=<span class="number">0.2</span>),</span><br><span class="line">        Dense(<span class="number">1024</span>),</span><br><span class="line">        LeakyReLU(alpha=<span class="number">0.2</span>),</span><br><span class="line">        Dense(<span class="number">784</span>, activation=<span class="string">&quot;tanh&quot;</span>)</span><br><span class="line">    ], name=<span class="string">&quot;Generator&quot;</span>)</span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>,</span><br><span class="line">                  optimizer=Adam(learning_rate=<span class="number">0.0002</span>, beta_1=<span class="number">0.5</span>))</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_discriminator</span>():</span><br><span class="line">    model = Sequential([</span><br><span class="line">        Dense(<span class="number">1024</span>, input_dim=<span class="number">784</span>),<span class="comment">#输入的是784维</span></span><br><span class="line">        LeakyReLU(alpha=<span class="number">0.2</span>),</span><br><span class="line">        Dropout(<span class="number">0.3</span>),</span><br><span class="line">        Dense(<span class="number">512</span>),</span><br><span class="line">        LeakyReLU(alpha=<span class="number">0.2</span>),</span><br><span class="line">        Dropout(<span class="number">0.3</span>),</span><br><span class="line">        Dense(<span class="number">256</span>),</span><br><span class="line">        LeakyReLU(alpha=<span class="number">0.2</span>),</span><br><span class="line">        Dropout(<span class="number">0.3</span>),</span><br><span class="line">        Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>)</span><br><span class="line">    ], name=<span class="string">&quot;Discriminator&quot;</span>)</span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>,</span><br><span class="line">                  optimizer=Adam(learning_rate=<span class="number">0.0002</span>, beta_1=<span class="number">0.5</span>))</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_gan</span>(<span class="params">gen, disc</span>):</span><br><span class="line">    disc.trainable = <span class="literal">False</span><span class="comment">#冻结disc的训练</span></span><br><span class="line">    gan_input = Input(shape=(NOISE_DIM,))<span class="comment">#返回一个符号张量，进行声明</span></span><br><span class="line">    gan_output = disc(gen(gan_input))</span><br><span class="line">    gan = Model(gan_input, gan_output)</span><br><span class="line">    gan.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>,</span><br><span class="line">                optimizer=Adam(learning_rate=<span class="number">0.0002</span>, beta_1=<span class="number">0.5</span>))</span><br><span class="line">    <span class="keyword">return</span> gan</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 4. 采样可视化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_images</span>(<span class="params">gen, epoch, examples=<span class="number">25</span>, dim=(<span class="params"><span class="number">5</span>, <span class="number">5</span></span>), figsize=(<span class="params"><span class="number">6</span>, <span class="number">6</span></span>)</span>):</span><br><span class="line">    noise = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=(examples, NOISE_DIM))</span><br><span class="line">    gen_imgs = gen.predict(noise, verbose=<span class="number">0</span>).reshape(examples, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=figsize)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(examples):</span><br><span class="line">        plt.subplot(dim[<span class="number">0</span>], dim[<span class="number">1</span>], i+<span class="number">1</span>)</span><br><span class="line">        plt.imshow(gen_imgs[i], cmap=<span class="string">&quot;gray&quot;</span>)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.savefig(os.path.join(RESULT_DIR, <span class="string">f&quot;generated_epoch_<span class="subst">&#123;epoch:03d&#125;</span>.png&quot;</span>))</span><br><span class="line">    plt.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 5. 训练</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    x_train = load_data()</span><br><span class="line">    data_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line">    steps_per_epoch = data_size // BATCH_SIZE<span class="comment">#训练步数，（总数据量//批次大小）</span></span><br><span class="line"></span><br><span class="line">    gen        = build_generator()</span><br><span class="line">    disc       = build_discriminator()</span><br><span class="line">    gan        = build_gan(gen, disc)</span><br><span class="line"></span><br><span class="line">    real_labels = np.ones((BATCH_SIZE, <span class="number">1</span>))</span><br><span class="line">    fake_labels = np.zeros((BATCH_SIZE, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, EPOCHS+<span class="number">1</span>):<span class="comment">#批次迭代——所有训练样本都被模型看过一次 + 参数已更新若干次</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>/<span class="subst">&#123;EPOCHS&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(steps_per_epoch):<span class="comment">#步数迭代</span></span><br><span class="line">            <span class="comment"># ---- 训练判别器 ----</span></span><br><span class="line">            noise      = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=(BATCH_SIZE, NOISE_DIM))</span><br><span class="line">            fake_imgs  = gen.predict(noise, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            real_imgs  = x_train[np.random.randint(<span class="number">0</span>, data_size, BATCH_SIZE)]</span><br><span class="line"></span><br><span class="line">            disc.trainable = <span class="literal">True</span></span><br><span class="line">            d_loss_real = disc.train_on_batch(real_imgs, real_labels)</span><br><span class="line">            d_loss_fake = disc.train_on_batch(fake_imgs, fake_labels)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ---- 训练生成器 ----</span></span><br><span class="line">            disc.trainable = <span class="literal">False</span></span><br><span class="line">            g_loss = gan.train_on_batch(noise, real_labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每个 epoch 结束打印一次</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;  D loss real: <span class="subst">&#123;d_loss_real:<span class="number">.4</span>f&#125;</span>  fake: <span class="subst">&#123;d_loss_fake:<span class="number">.4</span>f&#125;</span>  &quot;</span></span><br><span class="line">              <span class="string">f&quot;G loss: <span class="subst">&#123;g_loss:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每 10 个 epoch 保存一次图片</span></span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">1</span> <span class="keyword">or</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            save_images(gen, epoch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练完毕保存最终权重</span></span><br><span class="line">    gen.save(os.path.join(RESULT_DIR, <span class="string">&quot;generator.h5&quot;</span>))</span><br><span class="line">    disc.save(os.path.join(RESULT_DIR, <span class="string">&quot;discriminator.h5&quot;</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training done. Results saved to ./result/&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    train()</span><br></pre></td></tr></table></figure>

<p>gen、disc生成用的是Sequential</p>
<p>gan用的是Model</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import、设参</span><br><span class="line">==&gt; load数据</span><br><span class="line">==&gt; build gen disc gan ，gen和disc用Sequential，gan用Model；最后都先.compile()进行预编译后（注册loss，并设置optimizer），再返回模型</span><br><span class="line">==&gt; 可视化，save保存</span><br><span class="line">==&gt; train，生成模型，数据，epoch迭代（更新、保存权重），steps_per_epoch迭代（损失更新，生成图片）</span><br></pre></td></tr></table></figure>



<h1 id="基于PyTorch"><a href="#基于PyTorch" class="headerlink" title="基于PyTorch"></a>基于PyTorch</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.datasets.mnist <span class="keyword">as</span> mnist</span><br><span class="line">mnist.MNIST.mirrors = [</span><br><span class="line">    <span class="string">&quot;https://mirrors.tuna.tsinghua.edu.cn/pytorch/mnist/&quot;</span>,  <span class="comment"># 清华镜像</span></span><br><span class="line">    <span class="string">&quot;https://ossci-datasets.s3.amazonaws.com/mnist/&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://storage.googleapis.com/cvdf-datasets/mnist/&quot;</span>,</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 设置随机种子以确保可重复性</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设备</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;mps&#x27;</span> <span class="keyword">if</span> torch.backends.mps.is_available() <span class="keyword">else</span> (<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成器网络</span></span><br><span class="line"><span class="comment"># 网络结构采用转置卷积的方式，逐步将低维噪声向量升维到图像大小</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, latent_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.latent_dim = latent_dim  <span class="comment"># 潜在空间维度，用于控制生成图像的多样性</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># 第一层：将潜在向量映射到高维特征空间</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, latent_dim)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 256*7*7)</span></span><br><span class="line">            nn.Linear(latent_dim, <span class="number">256</span> * <span class="number">7</span> * <span class="number">7</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">256</span> * <span class="number">7</span> * <span class="number">7</span>),  <span class="comment"># 批归一化有助于训练稳定性</span></span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),  <span class="comment"># 使用ReLU激活函数引入非线性</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 重塑为卷积层的输入格式</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 256*7*7)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 256, 7, 7)</span></span><br><span class="line">            nn.Flatten(<span class="number">0</span>, -<span class="number">1</span>),</span><br><span class="line">            nn.Unflatten(<span class="number">0</span>, (-<span class="number">1</span>, <span class="number">256</span>, <span class="number">7</span>, <span class="number">7</span>)),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 第一个转置卷积层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 256, 7, 7)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 128, 14, 14)</span></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 第二个转置卷积层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 128, 14, 14)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 64, 28, 28)</span></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 最后的卷积层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 64, 28, 28)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 1, 28, 28)</span></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判别器网络</span></span><br><span class="line"><span class="comment"># 判别器的作用是区分真实图像和生成器生成的假图像</span></span><br><span class="line"><span class="comment"># 网络结构使用卷积层逐步提取图像特征，最终输出二分类概率</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># 第一层卷积：提取基础图像特征</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 1, 28, 28) (MNIST图像尺寸)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 64, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),  <span class="comment"># LeakyReLU避免梯度消失</span></span><br><span class="line">            nn.Dropout2d(<span class="number">0.3</span>),  <span class="comment"># Dropout防止过拟合</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 第二层卷积</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 64, 14, 14)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 128, 7, 7)</span></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout2d(<span class="number">0.3</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 展平层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 128, 7, 7)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 128 * 7 * 7)</span></span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 全连接层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 128 * 7 * 7)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 1)</span></span><br><span class="line">            nn.Linear(<span class="number">128</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数设置</span></span><br><span class="line"><span class="comment"># 这些参数的选择对GAN的训练稳定性和生成效果有重要影响</span></span><br><span class="line">latent_dim = <span class="number">100</span>  <span class="comment"># 潜在空间维度，较大的维度有助于生成更多样的图像</span></span><br><span class="line">batch_size = <span class="number">64</span>   <span class="comment"># 批次大小，平衡训练效率和内存占用</span></span><br><span class="line">num_epochs = <span class="number">100</span>  <span class="comment"># 训练轮数</span></span><br><span class="line">lr = <span class="number">0.0002</span>      <span class="comment"># 学习率，GAN训练需要较小的学习率以保持稳定</span></span><br><span class="line">beta1 = <span class="number">0.5</span>      <span class="comment"># Adam优化器的beta1参数，较小的值有助于训练稳定</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载和预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataset = MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化网络</span></span><br><span class="line">generator = Generator(latent_dim).to(device)</span><br><span class="line">discriminator = Discriminator().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数和优化器</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line">g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br><span class="line">d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="comment"># GAN的训练过程是一个动态博弈过程，需要平衡生成器和判别器的训练</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> i, (real_images, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            batch_size = real_images.size(<span class="number">0</span>)</span><br><span class="line">            real_images = real_images.to(device)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 训练判别器</span></span><br><span class="line">            <span class="comment"># 判别器需要学会区分真实图像（标签为1）和生成图像（标签为0）</span></span><br><span class="line">            d_optimizer.zero_grad()  <span class="comment"># 清空判别器的梯度</span></span><br><span class="line">            label_real = torch.ones(batch_size, <span class="number">1</span>).to(device)    <span class="comment"># 真实图像的标签为1</span></span><br><span class="line">            label_fake = torch.zeros(batch_size, <span class="number">1</span>).to(device)   <span class="comment"># 生成图像的标签为0</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 计算判别器对真实图像的损失</span></span><br><span class="line">            output_real = discriminator(real_images)  <span class="comment"># 判别器对真实图像的预测结果</span></span><br><span class="line">            d_loss_real = criterion(output_real, label_real)  <span class="comment"># 计算真实图像的二元交叉熵损失</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 生成假图像并计算判别器的损失,依赖于随机种子</span></span><br><span class="line">            noise = torch.randn(batch_size, latent_dim).to(device)  <span class="comment"># 生成随机噪声</span></span><br><span class="line">            fake_images = generator(noise)  <span class="comment"># 使用生成器生成假图像</span></span><br><span class="line">            output_fake = discriminator(fake_images.detach())  <span class="comment"># detach()防止梯度传递到生成器</span></span><br><span class="line">            d_loss_fake = criterion(output_fake, label_fake)  <span class="comment"># 计算假图像的二元交叉熵损失</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算判别器的总损失并更新参数</span></span><br><span class="line">            d_loss = d_loss_real + d_loss_fake  <span class="comment"># 判别器总损失是真假图像损失之和</span></span><br><span class="line">            d_loss.backward()  <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">            d_optimizer.step()  <span class="comment"># 更新判别器参数</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 训练生成器</span></span><br><span class="line">            <span class="comment"># 生成器的目标是生成能够欺骗判别器的图像</span></span><br><span class="line">            g_optimizer.zero_grad()  <span class="comment"># 清空生成器的梯度</span></span><br><span class="line">            output_fake = discriminator(fake_images)  <span class="comment"># 判别器对生成图像的预测</span></span><br><span class="line">            <span class="comment"># 生成器的损失：希望判别器将生成的图像判断为真实图像</span></span><br><span class="line">            g_loss = criterion(output_fake, label_real)  <span class="comment"># 使用真实标签计算损失</span></span><br><span class="line"></span><br><span class="line">            g_loss.backward()  <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">            g_optimizer.step()  <span class="comment"># 更新生成器参数</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>] Batch [<span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(dataloader)&#125;</span>] &#x27;</span></span><br><span class="line">                      <span class="string">f&#x27;d_loss: <span class="subst">&#123;d_loss.item():<span class="number">.4</span>f&#125;</span> g_loss: <span class="subst">&#123;g_loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每个epoch保存生成的图像样本</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            save_fake_images(epoch + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存生成的图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_fake_images</span>(<span class="params">epoch</span>):</span><br><span class="line">    <span class="comment"># 保存生成器在当前epoch生成的图像样本</span></span><br><span class="line">    generator.<span class="built_in">eval</span>()  <span class="comment"># 将生成器设置为评估模式</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 不计算梯度，节省内存</span></span><br><span class="line">        noise = torch.randn(<span class="number">16</span>, latent_dim).to(device)  <span class="comment"># 生成16个随机噪声向量</span></span><br><span class="line">        fake_images = generator(noise)  <span class="comment"># 生成16张图像</span></span><br><span class="line">        fake_images = fake_images.cpu().numpy()  <span class="comment"># 将张量转换为NumPy数组</span></span><br><span class="line">        </span><br><span class="line">        plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line">            plt.subplot(<span class="number">4</span>, <span class="number">4</span>, i + <span class="number">1</span>)</span><br><span class="line">            plt.imshow(fake_images[i, <span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">            plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        plt.savefig(<span class="string">f&#x27;./reuslt1/fake_images_epoch_<span class="subst">&#123;epoch&#125;</span>.png&#x27;</span>)</span><br><span class="line">        plt.close()</span><br><span class="line">    generator.train()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><h3 id="模块导入"><a href="#模块导入" class="headerlink" title="模块导入"></a>模块导入</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><code>torch</code>：PyTorch核心</li>
<li><code>torchvision</code>：听MNIST数据集和图像变换</li>
<li><code>matplotlib</code>：可视化</li>
</ul>
<h3 id="设置随机种子-设备"><a href="#设置随机种子-设备" class="headerlink" title="设置随机种子&amp;设备"></a>设置随机种子&amp;设备</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(42)</span><br><span class="line"></span><br><span class="line"># 生成假图像并计算判别器的损失,依赖于随机种子</span><br><span class="line">noise = torch.randn(batch_size, latent_dim).to(device)  # 生成随机噪声</span><br></pre></td></tr></table></figure>

<h2 id="零件"><a href="#零件" class="headerlink" title="零件"></a>零件</h2><h3 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):<span class="comment">#继承</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, latent_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, <span class="variable language_">self</span>).__init__()<span class="comment">#调用父类nn.Module</span></span><br><span class="line">        <span class="variable language_">self</span>.latent_dim = latent_dim  <span class="comment"># 潜在空间维度（噪声长度），用于控制生成图像的多样性</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># 第一层：将潜在向量映射到高维特征空间</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, latent_dim)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 256*7*7)</span></span><br><span class="line">            nn.Linear(latent_dim, <span class="number">256</span> * <span class="number">7</span> * <span class="number">7</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">256</span> * <span class="number">7</span> * <span class="number">7</span>),  <span class="comment"># 批归一化有助于训练稳定性</span></span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),  <span class="comment"># 使用ReLU激活函数引入非线性</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 将 1D 噪声向量（如 [batch, 100]）映射到一个高维向量（[batch, 256×7×7 = 12544]）；后续转为4D</span></span><br><span class="line">           </span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 重塑为卷积层的输入格式</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 256*7*7)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 256, 7, 7)</span></span><br><span class="line">            nn.Flatten(<span class="number">0</span>, -<span class="number">1</span>),<span class="comment">#压为1D</span></span><br><span class="line">            nn.Unflatten(<span class="number">0</span>, (-<span class="number">1</span>, <span class="number">256</span>, <span class="number">7</span>, <span class="number">7</span>)),<span class="comment">#恢复为4D</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 第一个转置卷积层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 256, 7, 7)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 128, 14, 14)</span></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 第二个转置卷积层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 128, 14, 14)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 64, 28, 28)</span></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 最后的卷积层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 64, 28, 28)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 1, 28, 28)</span></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(z)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#forward 函数定义了数据在网络中“从输入到输出”的计算流程。当你调用 model(input) 时，PyTorch 会自动调用 forward(input)。 </span></span><br></pre></td></tr></table></figure>

<p><strong>DCGAN 设计准则</strong>：<br>生成器中，<strong>每个转置卷积后都跟 BatchNorm + ReLU</strong>（最后一层除外）。</p>
<p>归一化处理，再运行激活函数</p>
<p>PyTorch 内部会：</p>
<ol>
<li>检查 <code>generator</code> 是 <code>nn.Module</code> 的子类；</li>
<li>自动调用 <code>generator.forward(noise)</code>；</li>
<li>同时<strong>记录计算图（computation graph）</strong>，以便后续反向传播（<code>.backward()</code>）。</li>
</ol>
<h3 id="判别器"><a href="#判别器" class="headerlink" title="判别器"></a>判别器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 判别器网络</span></span><br><span class="line"><span class="comment"># 判别器的作用是区分真实图像和生成器生成的假图像</span></span><br><span class="line"><span class="comment"># 网络结构使用卷积层逐步提取图像特征，最终输出二分类概率</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># 第一层卷积：提取基础图像特征</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 1, 28, 28) (MNIST图像尺寸)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 64, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),  <span class="comment"># LeakyReLU避免梯度消失</span></span><br><span class="line">            nn.Dropout2d(<span class="number">0.3</span>),  <span class="comment"># Dropout防止过拟合</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 第二层卷积</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 64, 14, 14)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 128, 7, 7)</span></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout2d(<span class="number">0.3</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 展平层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 128, 7, 7)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 128 * 7 * 7)</span></span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 全连接层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 128 * 7 * 7)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 1)</span></span><br><span class="line">            nn.Linear(<span class="number">128</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()<span class="comment">#</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#从[B,1,28,28](图像) =&gt; [B,1](概率)</span></span><br><span class="line">    <span class="comment">#28×28 → 14×14 → 7×7</span></span><br><span class="line">    <span class="comment">#与生成器：7×7 → 14×14 → 28×28对称</span></span><br></pre></td></tr></table></figure>



<h2 id="预备"><a href="#预备" class="headerlink" title="预备"></a>预备</h2><h3 id="超参-1"><a href="#超参-1" class="headerlink" title="超参"></a>超参</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这些参数的选择对GAN的训练稳定性和生成效果有重要影响</span></span><br><span class="line">latent_dim = <span class="number">100</span>  <span class="comment"># 潜在空间维度，较大的维度有助于生成更多样的图像</span></span><br><span class="line">batch_size = <span class="number">64</span>   <span class="comment"># 批次大小，平衡训练效率和内存占用</span></span><br><span class="line">num_epochs = <span class="number">100</span>  <span class="comment"># 训练轮数</span></span><br><span class="line">lr = <span class="number">0.0002</span>      <span class="comment"># 学习率，GAN训练需要较小的学习率以保持稳定</span></span><br><span class="line">beta1 = <span class="number">0.5</span>      <span class="comment"># Adam优化器的beta1参数，较小的值有助于训练稳定</span></span><br></pre></td></tr></table></figure>

<h3 id="处理"><a href="#处理" class="headerlink" title="处理"></a>处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据加载和预处理</span></span><br><span class="line">transform = transforms.Compose([<span class="comment">#将多步图像变换操作合并</span></span><br><span class="line">    transforms.ToTensor(),<span class="comment">#将PIL图像（或NumPy数组）转换为Tensor；像素：[0,255] =&gt; [0.0,1.0]；输出为(C, H, W)</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))<span class="comment">#将[0.0, 1.0]数据映射到[-1.0, 1.0]——生成器的最后一层用Tanh()，输出范围是[-1,1]</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataset = MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化网络</span></span><br><span class="line">generator = Generator(latent_dim).to(device)</span><br><span class="line">discriminator = Discriminator().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数和优化器</span></span><br><span class="line">criterion = nn.BCELoss()<span class="comment">#损失函数</span></span><br><span class="line">g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br><span class="line">d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><code>DataLoader(...)</code></p>
<ul>
<li><strong>作用</strong>：将数据集封装成<strong>可迭代的批次（batches）</strong>，支持多线程、打乱等。</li>
<li>参数详解：<ul>
<li><code>dataset</code>：前面创建的 <code>MNIST</code> 对象。</li>
<li><code>batch_size=batch_size</code>（如 64）：每次迭代返回 64 张图像 + 标签。</li>
<li><code>shuffle=True</code>：<strong>每个 epoch 打乱数据顺序</strong>，防止模型记住 batch 顺序，提升泛化能力。</li>
</ul>
</li>
</ul>
<p><code>.to(device)</code></p>
<p>将模型参数（weights）和缓冲区（如 BN 的 running_mean）<strong>移动到指定设备</strong>（CPU &#x2F; CUDA &#x2F; MPS）。</p>
<p><code>optim.Adam</code></p>
<ul>
<li><p><code>generator.parameters()</code>：获取生成器所有可训练参数（weights + biases）</p>
</li>
<li><p><code>lr=0.0002</code>：学习率。<strong>GAN 通常用较小 lr</strong>（如 0.0002），避免训练震荡。</p>
</li>
<li><pre><code>betas=(beta1, 0.999)
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  ：</span><br><span class="line"></span><br><span class="line">  - `beta1=0.5`（你在前面定义）：**一阶动量衰减率**（默认 0.9，但 DCGAN 推荐 0.5）</span><br><span class="line">  - `0.999`：二阶动量衰减率（默认值）</span><br><span class="line"></span><br><span class="line">## 训练</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">def train():</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        for i, (real_images, _) in enumerate(dataloader):#数据加载器返回batch</span><br><span class="line">            batch_size = real_images.size(0)#获取氮气batch实际大小</span><br><span class="line">            real_images = real_images.to(device)#</span><br><span class="line">            </span><br><span class="line">            # 训练判别器</span><br><span class="line">            # 判别器需要学会区分真实图像（标签为1）和生成图像（标签为0）</span><br><span class="line">            d_optimizer.zero_grad()  # 清空判别器的梯度</span><br><span class="line">            label_real = torch.ones(batch_size, 1).to(device)    # 真实图像的标签为1</span><br><span class="line">            label_fake = torch.zeros(batch_size, 1).to(device)   # 生成图像的标签为0</span><br><span class="line">            </span><br><span class="line">            # 计算判别器对真实图像的损失</span><br><span class="line">            output_real = discriminator(real_images)  # 判别器对真实图像的预测结果</span><br><span class="line">            d_loss_real = criterion(output_real, label_real)  # 计算真实图像的二元交叉熵损失</span><br><span class="line"></span><br><span class="line">            # 生成假图像并计算判别器的损失</span><br><span class="line">            noise = torch.randn(batch_size, latent_dim).to(device)  # 生成随机噪声</span><br><span class="line">            fake_images = generator(noise)  # 使用生成器生成假图像</span><br><span class="line">            output_fake = discriminator(fake_images.detach())  # detach()防止梯度传递到生成器</span><br><span class="line">            d_loss_fake = criterion(output_fake, label_fake)  # 计算假图像的二元交叉熵损失</span><br><span class="line"></span><br><span class="line">            # 计算判别器的总损失并更新参数</span><br><span class="line">            d_loss = d_loss_real + d_loss_fake  # 判别器总损失是真假图像损失之和</span><br><span class="line">            d_loss.backward()  # 反向传播计算梯度</span><br><span class="line">            d_optimizer.step()  # 更新判别器参数</span><br><span class="line">            </span><br><span class="line">            # 训练生成器</span><br><span class="line">            # 生成器的目标是生成能够欺骗判别器的图像</span><br><span class="line">            g_optimizer.zero_grad()  # 清空生成器的梯度</span><br><span class="line">            output_fake = discriminator(fake_images)  # 判别器对生成图像的预测</span><br><span class="line">            # 生成器的损失：希望判别器将生成的图像判断为真实图像</span><br><span class="line">            g_loss = criterion(output_fake, label_real)  # 使用真实标签计算损失</span><br><span class="line"></span><br><span class="line">            g_loss.backward()  # 反向传播计算梯度</span><br><span class="line">            g_optimizer.step()  # 更新生成器参数</span><br><span class="line">            </span><br><span class="line">            if i % 100 == 0:</span><br><span class="line">                print(f&#x27;Epoch [&#123;epoch&#125;/&#123;num_epochs&#125;] Batch [&#123;i&#125;/&#123;len(dataloader)&#125;] &#x27;</span><br><span class="line">                      f&#x27;d_loss: &#123;d_loss.item():.4f&#125; g_loss: &#123;g_loss.item():.4f&#125;&#x27;)</span><br><span class="line">        </span><br><span class="line">        # 每个epoch保存生成的图像样本</span><br><span class="line">        if (epoch + 1) % 10 == 0:</span><br><span class="line">            save_fake_images(epoch + 1)</span><br><span class="line"></span><br><span class="line">        #计算损失，.backward()反向传播，x_optimizer.step()更新参数</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<p><code>x_optimizer.zero_grad()</code> </p>
<p>PyTorch 默认<strong>累加梯度</strong>（用于梯度裁剪等），但每次更新前必须清零，否则梯度会错误累积。</p>
<p>损失函数criterion，来计算差异，同时进行反向传播计算梯度，更新参数</p>
<h2 id="save"><a href="#save" class="headerlink" title="save"></a>save</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">save_fake_images</span>(<span class="params">epoch</span>):</span><br><span class="line">    <span class="comment"># 保存生成器在当前epoch生成的图像样本</span></span><br><span class="line">    generator.<span class="built_in">eval</span>()  <span class="comment"># 将生成器设置为评估模式</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 不计算梯度，节省内存</span></span><br><span class="line">        noise = torch.randn(<span class="number">16</span>, latent_dim).to(device)  <span class="comment"># 生成16个随机噪声向量</span></span><br><span class="line">        fake_images = generator(noise)  <span class="comment"># 生成16张图像</span></span><br><span class="line">        fake_images = fake_images.cpu().numpy()  <span class="comment"># 将张量转换为NumPy数组</span></span><br><span class="line">        </span><br><span class="line">        plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))<span class="comment">#创建4*4英寸画布</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line">            plt.subplot(<span class="number">4</span>, <span class="number">4</span>, i + <span class="number">1</span>)<span class="comment">#分为4*4网格，选第i+1个子图</span></span><br><span class="line">            plt.imshow(fake_images[i, <span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">            plt.axis(<span class="string">&#x27;off&#x27;</span>)<span class="comment">#隐藏坐标轴</span></span><br><span class="line">        </span><br><span class="line">        plt.savefig(<span class="string">f&#x27;fake_images_epoch_<span class="subst">&#123;epoch&#125;</span>.png&#x27;</span>)</span><br><span class="line">        plt.close()</span><br><span class="line">    generator.train()<span class="comment">#恢复G的训练，与eval()对应</span></span><br></pre></td></tr></table></figure>



<h2 id="loss"><a href="#loss" class="headerlink" title="loss"></a>loss</h2><p>一个标量张量</p>
<p>通过损失函数来生成loss（<strong>差距</strong>）</p>
<p><code>d_loss_real = criterion(output_real, label_real)</code>——判别器输出接近1（真），loss近似0（label_real&#x3D;1）；反之，近似1</p>
<p><code>d_loss_fake = criterion(output_fake, label_fake)</code>——判别器输出接近0（假），loss近似0（label_fake&#x3D;0）；反之，近似1</p>
<p><code>g_loss = criterion(output_fake, label_real)</code>——同样，loss近似为0时，说明很逼真了</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.datasets.mnist <span class="keyword">as</span> mnist</span><br><span class="line">mnist.MNIST.mirrors = [</span><br><span class="line">    <span class="string">&quot;https://mirrors.tuna.tsinghua.edu.cn/pytorch/mnist/&quot;</span>,  <span class="comment"># 清华镜像</span></span><br><span class="line">    <span class="string">&quot;https://ossci-datasets.s3.amazonaws.com/mnist/&quot;</span>,</span><br><span class="line">    <span class="string">&quot;https://storage.googleapis.com/cvdf-datasets/mnist/&quot;</span>,</span><br><span class="line">]</span><br><span class="line"><span class="comment"># 设置随机种子以确保可重复性</span></span><br><span class="line">torch.manual_seed(<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置设备</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;mps&#x27;</span> <span class="keyword">if</span> torch.backends.mps.is_available() <span class="keyword">else</span> (<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成器网络</span></span><br><span class="line"><span class="comment"># 生成器的作用是将随机噪声转换为逼真的图像</span></span><br><span class="line"><span class="comment"># 网络结构采用转置卷积的方式，逐步将低维噪声向量升维到图像大小</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, latent_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(Generator, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.latent_dim = latent_dim  <span class="comment"># 潜在空间维度，用于控制生成图像的多样性</span></span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># 第一层：将潜在向量映射到高维特征空间</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, latent_dim)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 256*7*7)</span></span><br><span class="line">            nn.Linear(latent_dim, <span class="number">256</span> * <span class="number">7</span> * <span class="number">7</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">256</span> * <span class="number">7</span> * <span class="number">7</span>),  <span class="comment"># 批归一化有助于训练稳定性</span></span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),  <span class="comment"># 使用ReLU激活函数引入非线性</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 重塑为卷积层的输入格式</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 256*7*7)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 256, 7, 7)</span></span><br><span class="line">            nn.Flatten(<span class="number">0</span>, -<span class="number">1</span>),</span><br><span class="line">            nn.Unflatten(<span class="number">0</span>, (-<span class="number">1</span>, <span class="number">256</span>, <span class="number">7</span>, <span class="number">7</span>)),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 第一个转置卷积层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 256, 7, 7)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 128, 14, 14)</span></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 第二个转置卷积层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 128, 14, 14)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 64, 28, 28)</span></span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 最后的卷积层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 64, 28, 28)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 1, 28, 28)</span></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, z</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(z)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判别器网络</span></span><br><span class="line"><span class="comment"># 判别器的作用是区分真实图像和生成器生成的假图像</span></span><br><span class="line"><span class="comment"># 网络结构使用卷积层逐步提取图像特征，最终输出二分类概率</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Discriminator</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Discriminator, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.main = nn.Sequential(</span><br><span class="line">            <span class="comment"># 第一层卷积：提取基础图像特征</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 1, 28, 28) (MNIST图像尺寸)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 64, 14, 14)</span></span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),  <span class="comment"># LeakyReLU避免梯度消失</span></span><br><span class="line">            nn.Dropout2d(<span class="number">0.3</span>),  <span class="comment"># Dropout防止过拟合</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 第二层卷积</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 64, 14, 14)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 128, 7, 7)</span></span><br><span class="line">            nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout2d(<span class="number">0.3</span>),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 展平层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 128, 7, 7)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 128 * 7 * 7)</span></span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 全连接层</span></span><br><span class="line">            <span class="comment"># 输入shape: (batch_size, 128 * 7 * 7)</span></span><br><span class="line">            <span class="comment"># 输出shape: (batch_size, 1)</span></span><br><span class="line">            nn.Linear(<span class="number">128</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.main(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数设置</span></span><br><span class="line"><span class="comment"># 这些参数的选择对GAN的训练稳定性和生成效果有重要影响</span></span><br><span class="line">latent_dim = <span class="number">100</span>  <span class="comment"># 潜在空间维度，较大的维度有助于生成更多样的图像</span></span><br><span class="line">batch_size = <span class="number">64</span>   <span class="comment"># 批次大小，平衡训练效率和内存占用</span></span><br><span class="line">num_epochs = <span class="number">100</span>  <span class="comment"># 训练轮数</span></span><br><span class="line">lr = <span class="number">0.0002</span>      <span class="comment"># 学习率，GAN训练需要较小的学习率以保持稳定</span></span><br><span class="line">beta1 = <span class="number">0.5</span>      <span class="comment"># Adam优化器的beta1参数，较小的值有助于训练稳定</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据加载和预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">dataset = MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化网络</span></span><br><span class="line">generator = Generator(latent_dim).to(device)</span><br><span class="line">discriminator = Discriminator().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数和优化器</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line">g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br><span class="line">d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="comment"># GAN的训练过程是一个动态博弈过程，需要平衡生成器和判别器的训练</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> i, (real_images, _) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            batch_size = real_images.size(<span class="number">0</span>)</span><br><span class="line">            real_images = real_images.to(device)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 训练判别器</span></span><br><span class="line">            <span class="comment"># 判别器需要学会区分真实图像（标签为1）和生成图像（标签为0）</span></span><br><span class="line">            d_optimizer.zero_grad()  <span class="comment"># 清空判别器的梯度</span></span><br><span class="line">            label_real = torch.ones(batch_size, <span class="number">1</span>).to(device)    <span class="comment"># 真实图像的标签为1</span></span><br><span class="line">            label_fake = torch.zeros(batch_size, <span class="number">1</span>).to(device)   <span class="comment"># 生成图像的标签为0</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 计算判别器对真实图像的损失</span></span><br><span class="line">            output_real = discriminator(real_images)  <span class="comment"># 判别器对真实图像的预测结果</span></span><br><span class="line">            d_loss_real = criterion(output_real, label_real)  <span class="comment"># 计算真实图像的二元交叉熵损失</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 生成假图像并计算判别器的损失</span></span><br><span class="line">            noise = torch.randn(batch_size, latent_dim).to(device)  <span class="comment"># 生成随机噪声</span></span><br><span class="line">            fake_images = generator(noise)  <span class="comment"># 使用生成器生成假图像</span></span><br><span class="line">            output_fake = discriminator(fake_images.detach())  <span class="comment"># detach()防止梯度传递到生成器</span></span><br><span class="line">            d_loss_fake = criterion(output_fake, label_fake)  <span class="comment"># 计算假图像的二元交叉熵损失</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算判别器的总损失并更新参数</span></span><br><span class="line">            d_loss = d_loss_real + d_loss_fake  <span class="comment"># 判别器总损失是真假图像损失之和</span></span><br><span class="line">            d_loss.backward()  <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">            d_optimizer.step()  <span class="comment"># 更新判别器参数</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 训练生成器</span></span><br><span class="line">            <span class="comment"># 生成器的目标是生成能够欺骗判别器的图像</span></span><br><span class="line">            g_optimizer.zero_grad()  <span class="comment"># 清空生成器的梯度</span></span><br><span class="line">            output_fake = discriminator(fake_images)  <span class="comment"># 判别器对生成图像的预测</span></span><br><span class="line">            <span class="comment"># 生成器的损失：希望判别器将生成的图像判断为真实图像</span></span><br><span class="line">            g_loss = criterion(output_fake, label_real)  <span class="comment"># 使用真实标签计算损失</span></span><br><span class="line"></span><br><span class="line">            g_loss.backward()  <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">            g_optimizer.step()  <span class="comment"># 更新生成器参数</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&#x27;Epoch [<span class="subst">&#123;epoch&#125;</span>/<span class="subst">&#123;num_epochs&#125;</span>] Batch [<span class="subst">&#123;i&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(dataloader)&#125;</span>] &#x27;</span></span><br><span class="line">                      <span class="string">f&#x27;d_loss: <span class="subst">&#123;d_loss.item():<span class="number">.4</span>f&#125;</span> g_loss: <span class="subst">&#123;g_loss.item():<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 每个epoch保存生成的图像样本</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            save_fake_images(epoch + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存生成的图像</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save_fake_images</span>(<span class="params">epoch</span>):</span><br><span class="line">    <span class="comment"># 保存生成器在当前epoch生成的图像样本</span></span><br><span class="line">    generator.<span class="built_in">eval</span>()  <span class="comment"># 将生成器设置为评估模式</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 不计算梯度，节省内存</span></span><br><span class="line">        noise = torch.randn(<span class="number">16</span>, latent_dim).to(device)  <span class="comment"># 生成16个随机噪声向量</span></span><br><span class="line">        fake_images = generator(noise)  <span class="comment"># 生成16张图像</span></span><br><span class="line">        fake_images = fake_images.cpu().numpy()  <span class="comment"># 将张量转换为NumPy数组</span></span><br><span class="line">        </span><br><span class="line">        plt.figure(figsize=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line">            plt.subplot(<span class="number">4</span>, <span class="number">4</span>, i + <span class="number">1</span>)</span><br><span class="line">            plt.imshow(fake_images[i, <span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">            plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        plt.savefig(<span class="string">f&#x27;fake_images_epoch_<span class="subst">&#123;epoch&#125;</span>.png&#x27;</span>)</span><br><span class="line">        plt.close()</span><br><span class="line">    generator.train()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train()</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>代码参考链接<a target="_blank" rel="noopener" href="https://blog.csdn.net/Chuck0415/article/details/146824611">使用PyTorch实现MNIST数据集的GAN网络_gan mnist-CSDN博客</a></p>
<p>（代码有变动）</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">mervin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/10/17/GAN-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/">http://example.com/2025/10/17/GAN-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">have a good time!</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E4%BC%AA%E9%80%A0/">深度伪造</a></div><div class="post-share"><div class="social-share" data-image="https://p111223.github.io/imgs/2025-05-01 005434.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/09/25/ssti%E5%86%85%E5%AD%98%E9%A9%AC/" title="ssti内存马"><img class="cover" src="https://p111223.github.io/imgs/2025-05-01 010058.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">ssti内存马</div></div><div class="info-2"><div class="info-item-1">Flask环境（旧版）#1app.add_url_rule() &#x2F;&#x2F;创实现一个路由 12345678910111213141516171819202122Flask.add_url_rule can now also register a view function.——Version 0.2The endpoint for the Module.add_url_rule method is now optional to be consistent with the function of the same name on the application object.——Version 0.6Flask no longer internally depends on rules being added through the add_url_rule function and can now also accept regular werkzeug rules added to the url map.——Version 0.7Flask...</div></div></div></a><a class="pagination-related" href="/2025/11/25/python%E5%8E%9F%E5%9E%8B%E9%93%BE%E6%B1%A1%E6%9F%93/" title="python原型链污染"><img class="cover" src="https://p111223.github.io/imgs/2025-05-19 225723.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">python原型链污染</div></div><div class="info-2"><div class="info-item-1">python原型链污染 不是所有类的属性都可以被污染的； 对所有类的方法无效   [!IMPORTANT] ——注： 我们只能更改对应区域的参数内容**(改属性、变量)**，并不能自己实现命令的执行； 要有其他区域来协助实现回显&#x2F;执行。  合并函数——实现污染的一种方式 123456789101112131415def merge(src, dst):    # Recursive merge function    for k, v in src.items():        if hasattr(dst, &#x27;__getitem__&#x27;):            if dst.get(k) and type(v) == dict:                merge(v, dst.get(k))            else:                dst[k] = v        elif hasattr(dst, k) and type(v) == dict:            merge(v, getattr(dst,...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">mervin</div><div class="author-info-description">欢迎来到mervin的个人博客</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/p111223" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:3776130943@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8Etensorflow"><span class="toc-number">1.</span> <span class="toc-text">基于tensorflow</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BE%9D%E8%B5%96"><span class="toc-number">1.1.</span> <span class="toc-text">依赖</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B6%85%E5%8F%82"><span class="toc-number">1.2.</span> <span class="toc-text">超参</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.</span> <span class="toc-text">数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E4%BB%B6"><span class="toc-number">1.4.</span> <span class="toc-text">模型构件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#build-generator"><span class="toc-number">1.4.1.</span> <span class="toc-text">build_generator()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#build-discriminator"><span class="toc-number">1.4.2.</span> <span class="toc-text">build_discriminator()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#build-gan"><span class="toc-number">1.4.3.</span> <span class="toc-text">build_gan</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.5.</span> <span class="toc-text">可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#train"><span class="toc-number">1.6.</span> <span class="toc-text">train()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C"><span class="toc-number">1.7.</span> <span class="toc-text">运行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8EPyTorch"><span class="toc-number">2.</span> <span class="toc-text">基于PyTorch</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E6%8F%90"><span class="toc-number">2.1.</span> <span class="toc-text">前提</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9D%97%E5%AF%BC%E5%85%A5"><span class="toc-number">2.1.1.</span> <span class="toc-text">模块导入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90-%E8%AE%BE%E5%A4%87"><span class="toc-number">2.1.2.</span> <span class="toc-text">设置随机种子&amp;设备</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%B6%E4%BB%B6"><span class="toc-number">2.2.</span> <span class="toc-text">零件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%99%A8"><span class="toc-number">2.2.1.</span> <span class="toc-text">生成器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A4%E5%88%AB%E5%99%A8"><span class="toc-number">2.2.2.</span> <span class="toc-text">判别器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E5%A4%87"><span class="toc-number">2.3.</span> <span class="toc-text">预备</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B6%85%E5%8F%82-1"><span class="toc-number">2.3.1.</span> <span class="toc-text">超参</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86"><span class="toc-number">2.3.2.</span> <span class="toc-text">处理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#save"><span class="toc-number">2.4.</span> <span class="toc-text">save</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#loss"><span class="toc-number">2.5.</span> <span class="toc-text">loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">2.6.</span> <span class="toc-text">代码</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2026/01/20/%E5%9F%BA%E4%BA%8EMCP%E7%9A%84%E8%BD%BB%E9%87%8F%E7%BA%A7AI%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8%E7%B3%BB%E7%BB%9F/" title="基于MCP的轻量级AI工具调用系统"><img src="https://p111223.github.io/imgs/2025-05-01 005957.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="基于MCP的轻量级AI工具调用系统"/></a><div class="content"><a class="title" href="/2026/01/20/%E5%9F%BA%E4%BA%8EMCP%E7%9A%84%E8%BD%BB%E9%87%8F%E7%BA%A7AI%E5%B7%A5%E5%85%B7%E8%B0%83%E7%94%A8%E7%B3%BB%E7%BB%9F/" title="基于MCP的轻量级AI工具调用系统">基于MCP的轻量级AI工具调用系统</a><time datetime="2026-01-20T14:11:31.000Z" title="发表于 2026-01-20 22:11:31">2026-01-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/11/25/python%E5%8E%9F%E5%9E%8B%E9%93%BE%E6%B1%A1%E6%9F%93/" title="python原型链污染"><img src="https://p111223.github.io/imgs/2025-05-19 225723.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="python原型链污染"/></a><div class="content"><a class="title" href="/2025/11/25/python%E5%8E%9F%E5%9E%8B%E9%93%BE%E6%B1%A1%E6%9F%93/" title="python原型链污染">python原型链污染</a><time datetime="2025-11-25T12:47:32.000Z" title="发表于 2025-11-25 20:47:32">2025-11-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/10/17/GAN-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/" title="GAN_代码生成"><img src="https://p111223.github.io/imgs/2025-05-01 005434.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GAN_代码生成"/></a><div class="content"><a class="title" href="/2025/10/17/GAN-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/" title="GAN_代码生成">GAN_代码生成</a><time datetime="2025-10-17T07:05:24.000Z" title="发表于 2025-10-17 15:05:24">2025-10-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/25/ssti%E5%86%85%E5%AD%98%E9%A9%AC/" title="ssti内存马"><img src="https://p111223.github.io/imgs/2025-05-01 010058.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ssti内存马"/></a><div class="content"><a class="title" href="/2025/09/25/ssti%E5%86%85%E5%AD%98%E9%A9%AC/" title="ssti内存马">ssti内存马</a><time datetime="2025-09-25T02:40:29.000Z" title="发表于 2025-09-25 10:40:29">2025-09-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/06/17/%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/" title="反序列化"><img src="https://p111223.github.io/imgs/2025-05-19 225326.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="反序列化"/></a><div class="content"><a class="title" href="/2025/06/17/%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96/" title="反序列化">反序列化</a><time datetime="2025-06-17T11:59:15.000Z" title="发表于 2025-06-17 19:59:15">2025-06-17</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(https://p111223.github.io/imgs/2025-05-01 005434.png);"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2026 By mervin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>